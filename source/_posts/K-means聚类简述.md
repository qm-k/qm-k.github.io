---
title: K-means聚类简述
date: 2020-03-17 14:31:44
tags:
- 算法
categories:
- 学习
- 算法
- ML
mathjax: true
---
# 简述
k-means聚类是一种典型的无监督学习，很多时候我们没有足够多的先验数据，无法对已有的数据进行充足的标注，这个时候无监督学习就能帮助我们在无法预知结果时接近问题。  
<!--more-->
而聚类算法简单来讲就是把一些数据划分成几个具备相似特征的组，每个组中的数据之间具备比其他组中数据跟高的关联性，也就是把特征相近的数据分入同一个组中。  
# K
在K-means聚类中，我们会把所有的数据划分为K个簇（cluster）（怎么确定K是多少呢？随缘试吧），得到K个数据组的中心。  
那么我们如何得到呢？其实很简单：  
1. 随机在数据域中生成K个初始均值。
2. 计算每一个数据到这几个初始均值的距离。按照距离的远近将每一个数据划分到对应的簇中，此时可以得到K个簇。
3. 计算每个簇的形心，得到新的均值。
4. 重复2、3步骤，直到收敛。  

K-means面对的第一个问题是如何保证收敛，前面的算法中强调结束条件就是收敛，可以证明的是K-means完全可以保证收敛性。下面我们定性的描述一下收敛性，我们定义其损失函数如下：  
$$
J = \sum_{j=1}^k \sum_{i=1}^n || x_i^{(j)} - c_j ||^2
$$
式子中：  
- k：簇的个数
- n：簇内点数
- $x_i^{(j)}$:簇内第$i$个点
- $c_j$:第$i$个簇的形心

这样就能求出数据域中所有数据到现有的形心之间的平方误差，而K-means的目标就是使得总体的群体内方差最小。所以该目标函数就可以作为其训练的终止条件。  

但是由于畸变函数J是非凸函数，意味着我们不能保证取得的最小值是全局最小值，也就是说k-means的结果会受到初值选取的影响。但如果你怕陷入局部最优，那么可以选取不同的初始值跑多遍k-means，这个时候就有了k-means++。  
## k-means++
k-means++原理也很简单，主要在于初始化中心和多次判断上。  
k-means++会多次初始化中心，每次都从输入的数据集中选取一个作为数据集的中心，然后再重复k-means的流程直到单次收敛。收敛之后再选取数据集中距离每一个形心最远的数据作为新的初始中心，再重复k-means的流程。  
还有关于大样本优化的k-means算法等等这里不再详说，可以参考https://www.cnblogs.com/pinard/p/6164214.html。  

关于k-means有一个很有趣的描述：  
<blockquote >有四个牧师去郊区布道，一开始牧师们随意选了几个布道点，并且把这几个布道点的情况公告给了郊区所有的居民，于是每个居民到离自己家最近的布道点去听课。

听课之后，大家觉得距离太远了，于是每个牧师统计了一下自己的课上所有的居民的地址，搬到了所有地址的中心地带，并且在海报上更新了自己的布道点的位置。
牧师每一次移动不可能离所有人都更近，有的人发现A牧师移动以后自己还不如去B牧师处听课更近，于是每个居民又去了离自己最近的布道点……
就这样，牧师每个礼拜更新自己的位置，居民根据自己的情况选择布道点，最终稳定了下来。</blockquote>

